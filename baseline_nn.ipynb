{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import pandas as pd\n",
    "\n",
    "mat = scipy.io.loadmat(\"/disk/home/metabric/BRCA1View20000.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCGB2A2</th>\n",
       "      <th>MUCL1</th>\n",
       "      <th>SCGB1D2</th>\n",
       "      <th>PIP</th>\n",
       "      <th>LOC648852</th>\n",
       "      <th>DNAJA2</th>\n",
       "      <th>TFF3</th>\n",
       "      <th>S100P</th>\n",
       "      <th>CPB1</th>\n",
       "      <th>CEACAM6</th>\n",
       "      <th>...</th>\n",
       "      <th>LOC641311</th>\n",
       "      <th>LOC645307</th>\n",
       "      <th>IL1RAP</th>\n",
       "      <th>LOC647149</th>\n",
       "      <th>LOC642453</th>\n",
       "      <th>LOC652100</th>\n",
       "      <th>LOC646050</th>\n",
       "      <th>LOC644912</th>\n",
       "      <th>LOC652294</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.365374</td>\n",
       "      <td>5.610561</td>\n",
       "      <td>5.134799</td>\n",
       "      <td>5.735802</td>\n",
       "      <td>5.855315</td>\n",
       "      <td>5.443985</td>\n",
       "      <td>8.005882</td>\n",
       "      <td>5.615179</td>\n",
       "      <td>6.682096</td>\n",
       "      <td>5.756946</td>\n",
       "      <td>...</td>\n",
       "      <td>5.409726</td>\n",
       "      <td>5.420998</td>\n",
       "      <td>5.396492</td>\n",
       "      <td>5.358751</td>\n",
       "      <td>5.424770</td>\n",
       "      <td>5.411055</td>\n",
       "      <td>5.242460</td>\n",
       "      <td>5.197767</td>\n",
       "      <td>5.643537</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>14.330442</td>\n",
       "      <td>7.979139</td>\n",
       "      <td>10.244240</td>\n",
       "      <td>7.138279</td>\n",
       "      <td>9.890661</td>\n",
       "      <td>11.195867</td>\n",
       "      <td>13.511164</td>\n",
       "      <td>10.548596</td>\n",
       "      <td>5.491578</td>\n",
       "      <td>6.805606</td>\n",
       "      <td>...</td>\n",
       "      <td>5.213895</td>\n",
       "      <td>5.167652</td>\n",
       "      <td>5.273045</td>\n",
       "      <td>5.287980</td>\n",
       "      <td>5.398061</td>\n",
       "      <td>5.226934</td>\n",
       "      <td>5.396744</td>\n",
       "      <td>5.352158</td>\n",
       "      <td>5.811813</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14.388331</td>\n",
       "      <td>6.016715</td>\n",
       "      <td>12.164435</td>\n",
       "      <td>5.272691</td>\n",
       "      <td>6.468386</td>\n",
       "      <td>5.265518</td>\n",
       "      <td>13.386034</td>\n",
       "      <td>8.764722</td>\n",
       "      <td>7.331663</td>\n",
       "      <td>11.967719</td>\n",
       "      <td>...</td>\n",
       "      <td>5.707537</td>\n",
       "      <td>5.327874</td>\n",
       "      <td>5.549589</td>\n",
       "      <td>5.333051</td>\n",
       "      <td>5.368544</td>\n",
       "      <td>5.296182</td>\n",
       "      <td>5.694022</td>\n",
       "      <td>5.459744</td>\n",
       "      <td>5.269015</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13.111243</td>\n",
       "      <td>7.251862</td>\n",
       "      <td>8.389121</td>\n",
       "      <td>8.664654</td>\n",
       "      <td>10.378585</td>\n",
       "      <td>5.544646</td>\n",
       "      <td>11.159766</td>\n",
       "      <td>8.263224</td>\n",
       "      <td>5.497906</td>\n",
       "      <td>5.923506</td>\n",
       "      <td>...</td>\n",
       "      <td>5.351022</td>\n",
       "      <td>5.009093</td>\n",
       "      <td>5.345468</td>\n",
       "      <td>5.176322</td>\n",
       "      <td>5.326991</td>\n",
       "      <td>5.480259</td>\n",
       "      <td>5.292691</td>\n",
       "      <td>5.242160</td>\n",
       "      <td>5.635111</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12.243103</td>\n",
       "      <td>7.293489</td>\n",
       "      <td>10.919581</td>\n",
       "      <td>9.566137</td>\n",
       "      <td>8.503568</td>\n",
       "      <td>11.492583</td>\n",
       "      <td>11.531352</td>\n",
       "      <td>9.818938</td>\n",
       "      <td>9.443134</td>\n",
       "      <td>6.889670</td>\n",
       "      <td>...</td>\n",
       "      <td>5.320883</td>\n",
       "      <td>5.487783</td>\n",
       "      <td>5.479967</td>\n",
       "      <td>5.332074</td>\n",
       "      <td>5.273141</td>\n",
       "      <td>5.282872</td>\n",
       "      <td>5.222482</td>\n",
       "      <td>5.186136</td>\n",
       "      <td>5.616094</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2128</td>\n",
       "      <td>7.199176</td>\n",
       "      <td>5.708891</td>\n",
       "      <td>5.672914</td>\n",
       "      <td>14.464282</td>\n",
       "      <td>9.816897</td>\n",
       "      <td>5.604835</td>\n",
       "      <td>13.303832</td>\n",
       "      <td>6.391817</td>\n",
       "      <td>13.010996</td>\n",
       "      <td>8.294409</td>\n",
       "      <td>...</td>\n",
       "      <td>5.695733</td>\n",
       "      <td>5.363557</td>\n",
       "      <td>5.394837</td>\n",
       "      <td>5.442947</td>\n",
       "      <td>5.423763</td>\n",
       "      <td>5.558225</td>\n",
       "      <td>5.575000</td>\n",
       "      <td>5.636068</td>\n",
       "      <td>5.454823</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2129</td>\n",
       "      <td>10.354497</td>\n",
       "      <td>14.464282</td>\n",
       "      <td>7.633244</td>\n",
       "      <td>14.464282</td>\n",
       "      <td>11.294018</td>\n",
       "      <td>5.468214</td>\n",
       "      <td>10.886420</td>\n",
       "      <td>12.260033</td>\n",
       "      <td>5.662372</td>\n",
       "      <td>6.274380</td>\n",
       "      <td>...</td>\n",
       "      <td>5.537254</td>\n",
       "      <td>5.613097</td>\n",
       "      <td>5.254552</td>\n",
       "      <td>5.205904</td>\n",
       "      <td>5.486546</td>\n",
       "      <td>5.206480</td>\n",
       "      <td>5.161718</td>\n",
       "      <td>5.451296</td>\n",
       "      <td>5.215587</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>9.294401</td>\n",
       "      <td>13.703049</td>\n",
       "      <td>6.063617</td>\n",
       "      <td>10.063176</td>\n",
       "      <td>5.393440</td>\n",
       "      <td>5.529584</td>\n",
       "      <td>10.937517</td>\n",
       "      <td>12.220400</td>\n",
       "      <td>9.985646</td>\n",
       "      <td>8.370227</td>\n",
       "      <td>...</td>\n",
       "      <td>5.520596</td>\n",
       "      <td>5.572326</td>\n",
       "      <td>5.317225</td>\n",
       "      <td>5.178158</td>\n",
       "      <td>5.316757</td>\n",
       "      <td>5.210332</td>\n",
       "      <td>5.239428</td>\n",
       "      <td>5.395402</td>\n",
       "      <td>5.501265</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2131</td>\n",
       "      <td>14.464282</td>\n",
       "      <td>13.203708</td>\n",
       "      <td>14.169804</td>\n",
       "      <td>13.731721</td>\n",
       "      <td>12.478907</td>\n",
       "      <td>5.347654</td>\n",
       "      <td>14.464282</td>\n",
       "      <td>9.593148</td>\n",
       "      <td>6.165212</td>\n",
       "      <td>11.207149</td>\n",
       "      <td>...</td>\n",
       "      <td>5.650387</td>\n",
       "      <td>5.359733</td>\n",
       "      <td>5.374522</td>\n",
       "      <td>5.245155</td>\n",
       "      <td>5.453742</td>\n",
       "      <td>5.353135</td>\n",
       "      <td>5.235716</td>\n",
       "      <td>5.155543</td>\n",
       "      <td>5.442698</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2132</td>\n",
       "      <td>6.730752</td>\n",
       "      <td>6.879782</td>\n",
       "      <td>6.134414</td>\n",
       "      <td>5.446989</td>\n",
       "      <td>5.320249</td>\n",
       "      <td>10.904521</td>\n",
       "      <td>5.347355</td>\n",
       "      <td>12.784519</td>\n",
       "      <td>5.391796</td>\n",
       "      <td>6.048233</td>\n",
       "      <td>...</td>\n",
       "      <td>5.193949</td>\n",
       "      <td>5.300694</td>\n",
       "      <td>5.545144</td>\n",
       "      <td>5.143987</td>\n",
       "      <td>5.496168</td>\n",
       "      <td>5.236875</td>\n",
       "      <td>5.123631</td>\n",
       "      <td>5.210307</td>\n",
       "      <td>5.456472</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2133 rows Ã— 20001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SCGB2A2      MUCL1    SCGB1D2        PIP  LOC648852     DNAJA2  \\\n",
       "0      6.365374   5.610561   5.134799   5.735802   5.855315   5.443985   \n",
       "1     14.330442   7.979139  10.244240   7.138279   9.890661  11.195867   \n",
       "2     14.388331   6.016715  12.164435   5.272691   6.468386   5.265518   \n",
       "3     13.111243   7.251862   8.389121   8.664654  10.378585   5.544646   \n",
       "4     12.243103   7.293489  10.919581   9.566137   8.503568  11.492583   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2128   7.199176   5.708891   5.672914  14.464282   9.816897   5.604835   \n",
       "2129  10.354497  14.464282   7.633244  14.464282  11.294018   5.468214   \n",
       "2130   9.294401  13.703049   6.063617  10.063176   5.393440   5.529584   \n",
       "2131  14.464282  13.203708  14.169804  13.731721  12.478907   5.347654   \n",
       "2132   6.730752   6.879782   6.134414   5.446989   5.320249  10.904521   \n",
       "\n",
       "           TFF3      S100P       CPB1    CEACAM6  ...  LOC641311  LOC645307  \\\n",
       "0      8.005882   5.615179   6.682096   5.756946  ...   5.409726   5.420998   \n",
       "1     13.511164  10.548596   5.491578   6.805606  ...   5.213895   5.167652   \n",
       "2     13.386034   8.764722   7.331663  11.967719  ...   5.707537   5.327874   \n",
       "3     11.159766   8.263224   5.497906   5.923506  ...   5.351022   5.009093   \n",
       "4     11.531352   9.818938   9.443134   6.889670  ...   5.320883   5.487783   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2128  13.303832   6.391817  13.010996   8.294409  ...   5.695733   5.363557   \n",
       "2129  10.886420  12.260033   5.662372   6.274380  ...   5.537254   5.613097   \n",
       "2130  10.937517  12.220400   9.985646   8.370227  ...   5.520596   5.572326   \n",
       "2131  14.464282   9.593148   6.165212  11.207149  ...   5.650387   5.359733   \n",
       "2132   5.347355  12.784519   5.391796   6.048233  ...   5.193949   5.300694   \n",
       "\n",
       "        IL1RAP  LOC647149  LOC642453  LOC652100  LOC646050  LOC644912  \\\n",
       "0     5.396492   5.358751   5.424770   5.411055   5.242460   5.197767   \n",
       "1     5.273045   5.287980   5.398061   5.226934   5.396744   5.352158   \n",
       "2     5.549589   5.333051   5.368544   5.296182   5.694022   5.459744   \n",
       "3     5.345468   5.176322   5.326991   5.480259   5.292691   5.242160   \n",
       "4     5.479967   5.332074   5.273141   5.282872   5.222482   5.186136   \n",
       "...        ...        ...        ...        ...        ...        ...   \n",
       "2128  5.394837   5.442947   5.423763   5.558225   5.575000   5.636068   \n",
       "2129  5.254552   5.205904   5.486546   5.206480   5.161718   5.451296   \n",
       "2130  5.317225   5.178158   5.316757   5.210332   5.239428   5.395402   \n",
       "2131  5.374522   5.245155   5.453742   5.353135   5.235716   5.155543   \n",
       "2132  5.545144   5.143987   5.496168   5.236875   5.123631   5.210307   \n",
       "\n",
       "      LOC652294  label  \n",
       "0      5.643537      6  \n",
       "1      5.811813      4  \n",
       "2      5.269015      4  \n",
       "3      5.635111      5  \n",
       "4      5.616094      3  \n",
       "...         ...    ...  \n",
       "2128   5.454823      3  \n",
       "2129   5.215587      5  \n",
       "2130   5.501265      2  \n",
       "2131   5.442698      5  \n",
       "2132   5.456472      1  \n",
       "\n",
       "[2133 rows x 20001 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_labels = [g[0] for g in mat['gene'][0]]\n",
    "df = pd.DataFrame(mat['data'].transpose(), columns=gene_labels)\n",
    "# df['id'] = mat['id'][0]\n",
    "df['label'] = mat['targets']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# create dataloader\n",
    "class BaselineMetabricDataset(Dataset):\n",
    "    def __init__(self, mat_file):\n",
    "        mat = scipy.io.loadmat(mat_file)\n",
    "        gene_labels = [g[0] for g in mat['gene'][0]]\n",
    "        self.df = pd.DataFrame(mat['data'].transpose(), columns=gene_labels)\n",
    "        # df['id'] = mat['id'][0]\n",
    "        self.df['label'] = mat['targets']\n",
    "        self.df['label'] = self.df['label'] - 1\n",
    "        self.labels = self.df.pop('label')\n",
    "        self.num_classes = 6\n",
    "        \n",
    "        # maybe normalize the columns\n",
    "        self.tensors = {}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.tensors:\n",
    "            return self.tensors[idx]\n",
    "        label = torch.zeros(self.num_classes)\n",
    "        label[self.labels[idx]] = 1\n",
    "        sample = (torch.FloatTensor(self.df.iloc[idx, :].values), label)\n",
    "        self.tensors[idx] = sample\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = \"/disk/home/metabric/BRCA1View20000.mat\"\n",
    "met_dataset = BaselineMetabricDataset(mat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2133"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(met_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, test_split = torch.utils.data.random_split(met_dataset, [1900, 233])\n",
    "train_loader = DataLoader(train_split, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_split, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 33.4311408996582\n",
      "1 33.842437744140625\n",
      "2 29.600605010986328\n",
      "3 33.3281364440918\n",
      "4 33.29365921020508\n",
      "5 28.77070426940918\n",
      "6 30.155563354492188\n",
      "7 27.86324691772461\n",
      "8 30.065120697021484\n",
      "9 27.8597354888916\n",
      "10 27.394798278808594\n",
      "11 27.335826873779297\n",
      "12 27.84022331237793\n",
      "13 24.21585464477539\n",
      "14 26.190759658813477\n",
      "15 24.8209171295166\n",
      "16 27.13934326171875\n",
      "17 22.396656036376953\n",
      "18 25.534509658813477\n",
      "19 25.666337966918945\n",
      "20 23.255090713500977\n",
      "21 25.254640579223633\n",
      "22 25.548118591308594\n",
      "23 21.12644386291504\n",
      "24 22.975475311279297\n",
      "25 20.779685974121094\n",
      "26 20.59823989868164\n",
      "27 20.567575454711914\n",
      "28 22.910226821899414\n",
      "29 19.635847091674805\n",
      "30 24.309661865234375\n",
      "31 25.80802345275879\n",
      "32 19.79580307006836\n",
      "33 21.617877960205078\n",
      "34 19.828155517578125\n",
      "35 19.519474029541016\n",
      "36 23.325336456298828\n",
      "37 19.340192794799805\n",
      "38 20.15607261657715\n",
      "39 22.60725975036621\n",
      "40 21.07512092590332\n",
      "41 21.386354446411133\n",
      "42 22.29369354248047\n",
      "43 24.881759643554688\n",
      "44 19.816030502319336\n",
      "45 18.97570037841797\n",
      "46 22.79999542236328\n",
      "47 18.90020179748535\n",
      "48 22.61192512512207\n",
      "49 20.565032958984375\n",
      "50 17.68906593322754\n",
      "51 17.888484954833984\n",
      "52 20.612092971801758\n",
      "53 21.795589447021484\n",
      "54 19.506061553955078\n",
      "55 18.816951751708984\n",
      "56 19.776439666748047\n",
      "57 16.667966842651367\n",
      "58 18.573009490966797\n",
      "59 21.30344581604004\n",
      "60 21.01797866821289\n",
      "61 18.540653228759766\n",
      "62 17.66374969482422\n",
      "63 23.799882888793945\n",
      "64 16.6235294342041\n",
      "65 18.17536163330078\n",
      "66 20.29248046875\n",
      "67 19.469924926757812\n",
      "68 19.229480743408203\n",
      "69 18.307790756225586\n",
      "70 19.44610595703125\n",
      "71 22.9898624420166\n",
      "72 19.927377700805664\n",
      "73 18.626249313354492\n",
      "74 18.613380432128906\n",
      "75 19.729196548461914\n",
      "76 19.231977462768555\n",
      "77 21.7554988861084\n",
      "78 18.09891700744629\n",
      "79 16.56272315979004\n",
      "80 16.238805770874023\n",
      "81 13.892617225646973\n",
      "82 17.032054901123047\n",
      "83 18.29014015197754\n",
      "84 20.288455963134766\n",
      "85 16.657503128051758\n",
      "86 17.98558235168457\n",
      "87 16.952905654907227\n",
      "88 19.998353958129883\n",
      "89 16.268478393554688\n",
      "90 17.816242218017578\n",
      "91 15.433296203613281\n",
      "92 14.773362159729004\n",
      "93 18.415485382080078\n",
      "94 19.826139450073242\n",
      "95 22.31182861328125\n",
      "96 22.368511199951172\n",
      "97 16.459529876708984\n",
      "98 17.260347366333008\n",
      "99 17.614116668701172\n"
     ]
    }
   ],
   "source": [
    "IN_DIM = 20000\n",
    "OUT_DIM = 6\n",
    "LEARNING_RATE = 1e-5\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(IN_DIM, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, OUT_DIM)\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    loss = None\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(epoch, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 0, 1, 0, 5, 2, 3, 3, 2, 4, 3, 2, 0, 2, 2, 1, 2, 3, 2, 0, 0, 3, 2,\n",
      "        2, 0, 1, 3, 0, 2, 3, 1, 2, 3, 2, 2, 2, 3, 4, 1, 1, 1, 3, 1, 0, 2, 3, 2,\n",
      "        2, 4, 2, 5, 0, 3, 5, 1, 3, 0, 0, 2, 3, 2, 0, 4])\n",
      "tensor([1, 4, 0, 1, 0, 5, 2, 3, 3, 2, 4, 3, 2, 0, 2, 2, 2, 2, 2, 2, 0, 0, 3, 2,\n",
      "        3, 0, 1, 3, 0, 2, 1, 0, 3, 3, 3, 3, 2, 3, 4, 1, 1, 0, 3, 1, 0, 3, 3, 2,\n",
      "        2, 1, 3, 5, 0, 3, 5, 0, 3, 0, 0, 2, 3, 4, 0, 4])\n",
      "---------------------------------\n",
      "tensor([0, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 1, 3, 1, 2, 2, 2, 2, 1, 3, 3,\n",
      "        3, 2, 2, 0, 2, 3, 3, 1, 0, 3, 0, 0, 2, 1, 2, 2, 2, 2, 5, 3, 2, 2, 1, 0,\n",
      "        2, 3, 3, 1, 0, 3, 5, 2, 3, 2, 2, 2, 1, 1, 0, 0])\n",
      "tensor([0, 2, 2, 2, 2, 3, 3, 2, 1, 3, 1, 3, 3, 1, 1, 2, 0, 2, 0, 2, 2, 1, 3, 3,\n",
      "        3, 4, 4, 0, 2, 2, 3, 1, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1, 5, 1, 3, 2, 0, 0,\n",
      "        2, 3, 3, 1, 0, 3, 5, 2, 3, 2, 2, 4, 1, 0, 0, 0])\n",
      "---------------------------------\n",
      "tensor([2, 2, 4, 3, 5, 3, 3, 3, 2, 2, 3, 2, 4, 2, 1, 2, 4, 2, 2, 2, 2, 0, 2, 2,\n",
      "        0, 2, 2, 3, 1, 2, 3, 4, 2, 2, 1, 2, 2, 2, 0, 3, 0, 5, 4, 1, 5, 0, 3, 2,\n",
      "        2, 2, 5, 2, 2, 0, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3])\n",
      "tensor([2, 2, 4, 2, 5, 3, 3, 3, 2, 4, 3, 2, 0, 2, 1, 2, 1, 2, 3, 2, 2, 0, 2, 2,\n",
      "        0, 3, 3, 3, 1, 2, 3, 4, 2, 3, 4, 3, 2, 2, 0, 3, 0, 5, 4, 1, 5, 0, 3, 3,\n",
      "        4, 2, 5, 2, 2, 0, 3, 2, 2, 3, 2, 3, 3, 2, 2, 0])\n",
      "---------------------------------\n",
      "tensor([0, 4, 3, 2, 2, 2, 0, 2, 2, 3, 2, 5, 4, 2, 2, 2, 2, 2, 2, 4, 3, 1, 3, 2,\n",
      "        2, 2, 2, 2, 0, 2, 3, 3, 2, 2, 3, 3, 3, 0, 2, 0, 2])\n",
      "tensor([0, 4, 3, 3, 2, 2, 0, 3, 2, 2, 2, 5, 4, 1, 4, 2, 2, 1, 1, 2, 3, 1, 3, 2,\n",
      "        3, 2, 3, 4, 0, 2, 3, 3, 2, 3, 3, 3, 2, 0, 4, 0, 3])\n",
      "---------------------------------\n",
      "Accuracy: 171 / 233 73.39%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        predictions = model(X)\n",
    "        _, pred = torch.max(predictions.data, 1)\n",
    "        _, labeled = torch.max(y.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (pred == labeled).sum().item()\n",
    "        print(pred)\n",
    "        print(labeled)\n",
    "        print(\"---------------------------------\")\n",
    "print(f\"Accuracy: {correct} / {total} {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'baseline_model_v0.2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
