{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2133\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "mat = scipy.io.loadmat('BRCA1View20000.mat')\n",
    "mat = {k:v for k, v in mat.items() if k[0] != '_'}\n",
    "print(len(mat['targets']))\n",
    "#mat = {k: pd.Series(v[0]) for k, v in mat.items() if len(v) == 0}\n",
    "mat = {k: pd.Series(v.flatten()) for k, v in mat.items()}\n",
    "\n",
    "targets = pd.DataFrame.from_dict(mat).dropna()[['id', 'targets']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(targets['targets']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "url=\"https://metabric.s3-us-west-1.amazonaws.com/cat2vec/cat2vec_adj.csv\"\n",
    "s=requests.get(url).content\n",
    "c=pd.read_csv(io.StringIO(s.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "c['id'] = c['Patient ID'].apply(lambda x: float(x.split(\"-\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = c.merge(labels, on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[data.columns[1:-2]], data['targets'], test_size=0.33, random_state=42)\n",
    "X_train = torch.tensor(X_train.values).float()\n",
    "y_train = torch.tensor(y_train.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.tensor(X_test.values).float()\n",
    "y_test = torch.tensor(y_test.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class MyDataset(Dataset):\n",
    "  def __init__(self, values, labels):\n",
    "    super(MyDataset, self).__init__()\n",
    "    self.values = values\n",
    "    self.labels = labels\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.values)  # number of samples in the dataset\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.values[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataset = MyDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "testset = MyDataset(X_test, y_test)\n",
    "testloader = DataLoader(testset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network related parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1917   # The image size = 1x10312 = 10312\n",
    "hidden_size = 50       # The number of nodes at the hidden layer\n",
    "num_classes = 6       # The number of output classes. In this case, from 1 to 39\n",
    "num_epochs = 5         # The number of times entire dataset is trained\n",
    "batch_size = 5       # The size of input data took for one iteration\n",
    "learning_rate = 0.001  # The speed of convergence\n",
    "train_test_split = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()                    # Inherited from the parent class nn.Module\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer: 10312 (input data) -> 50 (hidden node)\n",
    "        self.relu = nn.ReLU()                          # Non-Linear ReLU Layer: max(0,x)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # 2nd Full-Connected Layer: 50 (hidden node) -> 39 (output class)\n",
    "    \n",
    "    def forward(self, x):                              # Forward pass: stacking each layer together\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [20/66], Loss: 1.6189\n",
      "Epoch [2/5], Step [20/66], Loss: 1.6227\n",
      "Epoch [3/5], Step [20/66], Loss: 1.6009\n",
      "Epoch [4/5], Step [20/66], Loss: 1.6205\n",
      "Epoch [5/5], Step [20/66], Loss: 1.6015\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "#for epoch in range(0,1):\n",
    "    for i, (images, labels) in enumerate(dataloader):   # Load a batch of images with its (index, data, class)\n",
    "        images = Variable(images)         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        #print(images.shape)\n",
    "        #print(labels)\n",
    "        #print(i)\n",
    "        labels = [i-1 for i in labels]\n",
    "        #print(torch.tensor(labels).reshape(-1, 1).flatten())\n",
    "        labels = Variable(torch.tensor(labels).reshape(-1, 1).flatten())\n",
    "        \n",
    "        optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "        outputs = model(images.float())                             # Forward pass: compute the output class given a image\n",
    "        #if epoch == num_epochs-1:\n",
    "        #    print(outputs, labels.long())\n",
    "        loss = criterion(outputs, labels.long())                 # Compute the loss: difference between the output class and the pre-given label\n",
    "        \n",
    "        loss.backward()                                   # Backward pass: compute the weight\n",
    "        optimizer.step()                                  # Optimizer: update the weights of hidden nodes\n",
    "\n",
    "        if (i+1) % 20 == 0:                              # Logging\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' %(epoch+1, num_epochs, i+1, len(dataset)//batch_size, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 34 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "all_predicted = []\n",
    "all_labels = []\n",
    "for i, (images, labels) in enumerate(testloader):\n",
    "    images = Variable(images)\n",
    "    outputs = model(images.float()) \n",
    "    labels = [i-1 for i in labels]\n",
    "    labels = torch.tensor(labels).reshape(-1, 1).flatten()\n",
    "    _, predicted = torch.max(outputs.data, 1)  # Choose the best class from the output: The class with the best score\n",
    "    total += labels.size(0)                    # Increment the total count\n",
    "    correct += (predicted == labels).sum()     # Increment the correct count\n",
    "    all_predicted += list(predicted)\n",
    "    #all_labels += (labels)\n",
    "    \n",
    "    \n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
