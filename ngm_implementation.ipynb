{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing graph links...\n",
      "Loading and processing graph nodes...\n",
      "Number of links: 44338\n",
      "Number of nodes: 19717\n",
      "Number of nodes belong to Class 1 4103\n",
      "Number of nodes belong to Class 2 7875\n",
      "Number of nodes belong to Class 3 7739\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "DATA_DIR = \"data/PubMed-Diabetes/\"\n",
    "EDGE_PATH = DATA_DIR + \"Pubmed-Diabetes.DIRECTED.cites.tab\"\n",
    "NODE_PATH = DATA_DIR + \"Pubmed-Diabetes.NODE.paper.tab\"\n",
    "TF_IDF_DIM = 500\n",
    "\n",
    "# Load and process graph links\n",
    "print(\"Loading and processing graph links...\")\n",
    "node_pairs = set()\n",
    "with open(EDGE_PATH, 'r') as f:\n",
    "    next(f)  # skip header\n",
    "    next(f)  # skip header\n",
    "    for line in f:\n",
    "        columns = line.split()\n",
    "        src = int(columns[1][6:])\n",
    "        dest = int(columns[3].strip()[6:])\n",
    "        node_pairs.add((src, dest))\n",
    "        \n",
    "# Load and process graph nodes\n",
    "print(\"Loading and processing graph nodes...\")\n",
    "node2vec = OrderedDict()\n",
    "node2label = dict()\n",
    "class_1 = list()\n",
    "class_2 = list()\n",
    "class_3 = list()\n",
    "with open(NODE_PATH, 'r') as f:\n",
    "    next(f)  # skip header\n",
    "    vocabs = [e.split(':')[1] for e in next(f).split()[1:]]\n",
    "    for line in f:\n",
    "        columns = line.split()\n",
    "        node = int(columns[0])\n",
    "        label = int(columns[1][-1])\n",
    "        tf_idf_vec = [0.0] * TF_IDF_DIM\n",
    "\n",
    "        for e in columns[2:-1]:\n",
    "            word, value = e.split('=')\n",
    "            tf_idf_vec[vocabs.index(word)] = float(value)\n",
    "\n",
    "        node2vec[node] = tf_idf_vec\n",
    "        node2label[node] = label - 1\n",
    "        if label == 1:\n",
    "            class_1.append(node)\n",
    "        elif label == 2:\n",
    "            class_2.append(node)\n",
    "        elif label == 3:\n",
    "            class_3.append(node)\n",
    "\n",
    "# Debug statistics\n",
    "print(\"Number of links:\", len(node_pairs))\n",
    "assert len(node2vec) == (len(class_1) + len(class_2) + len(class_3))\n",
    "print(\"Number of nodes:\", len(node2vec))\n",
    "print(\"Number of nodes belong to Class 1\", len(class_1))\n",
    "print(\"Number of nodes belong to Class 2\", len(class_2))\n",
    "print(\"Number of nodes belong to Class 3\", len(class_3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network related parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"model/\"\n",
    "TEST_SIZE = 1000\n",
    "SEED_NODES = 20\n",
    "NUM_CATEGORIES = 3\n",
    "\n",
    "ALPHA = 0.2\n",
    "HIDDEN_1_DIM = 250\n",
    "HIDDEN_2_DIM = 100\n",
    "\n",
    "NUM_EPOCH = 12\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important variables from previous cells: node_pairs, class_1, class_2, class_3\n",
    "test_nodes = class_1[-TEST_SIZE:] + class_2[-TEST_SIZE:] + class_3[-TEST_SIZE:]\n",
    "train_node_pairs = []\n",
    "for src, dest in node_pairs:\n",
    "    if not (src in test_nodes or dest in test_nodes):\n",
    "        train_node_pairs.append((src, dest))\n",
    "\n",
    "seed_nodes = class_1[:SEED_NODES] + class_2[:SEED_NODES] + class_3[:SEED_NODES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "class MY_NGM_FFNN(nn.Module):\n",
    "    def __init__(self, alpha, input_dim, hidden1_dim, hidden2_dim, output_dim, device=torch.device('cpu')):\n",
    "        super(MY_NGM_FFNN, self).__init__()\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.loss_function = nn.NLLLoss()\n",
    "\n",
    "        self.hidden1 = nn.Linear(input_dim, hidden1_dim)\n",
    "        self.hidden2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
    "        self.output = nn.Linear(hidden2_dim, output_dim)\n",
    "\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def save(self, output_dir, model_name):\n",
    "        print(\"Saving model...\")\n",
    "        torch.save(self.state_dict(), output_dir + model_name + \".pt\")\n",
    "        print(\"Model saved.\")\n",
    "\n",
    "    def load(self, output_dir, model_name):\n",
    "        print(\"Loading model...\")\n",
    "        self.load_state_dict(torch.load(output_dir + model_name + \".pt\"))\n",
    "        print(\"Model loaded.\")\n",
    "        \n",
    "    def forward(self, tf_idf_vec):\n",
    "        # First feed-forward layer\n",
    "        hidden1 = F.relu(self.hidden1(tf_idf_vec))\n",
    "        # Second feed-forward layer\n",
    "        hidden2 = F.relu(self.hidden2(hidden1))\n",
    "\n",
    "        # Output layer\n",
    "        return F.log_softmax(self.output(hidden2), -1)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.hidden1.reset_parameters()\n",
    "        self.hidden2.reset_parameters()\n",
    "        self.output.reset_parameters()\n",
    "    \n",
    "    def aggregate_ce(self, output, targets):\n",
    "        loss = 0\n",
    "        self.loss_function(output, targets)\n",
    "        #for o, t in zip(output, targets):\n",
    "        ##    print(o.size(), t.size())\n",
    "    #    loss += self.loss_function(o.view(1, -1), t)\n",
    "        return loss \n",
    "\n",
    "    \n",
    "    def train_(self, seed_nodes, train_node_pairs, node2vec, node2label, \n",
    "               num_epoch, batch_size, learning_rate):\n",
    "        print(\"Training...\")\n",
    "        self.train()\n",
    "\n",
    "        loss_function = nn.NLLLoss()\n",
    "        optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        node2neighbors = defaultdict(list)\n",
    "        for src, dest in train_node_pairs:\n",
    "            node2neighbors[src].append(dest)\n",
    "            node2neighbors[dest].append(src)\n",
    "            \n",
    "        labeled_nodes = dict()\n",
    "        for node in seed_nodes:\n",
    "            labeled_nodes[node] = node2label[node]\n",
    "\n",
    "        iteration = 1\n",
    "        while iteration < 3:\n",
    "            print(\"=\" * 80)\n",
    "            print(\"Generation: {} (with {} labeled nodes)\".format(iteration, len(labeled_nodes)))\n",
    "            iteration += 1\n",
    "\n",
    "            for e in range(NUM_EPOCH):\n",
    "                train_node_pairs_cpy =  train_node_pairs[:]\n",
    "                total_loss = 0\n",
    "                \n",
    "                while train_node_pairs_cpy:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = torch.tensor(0, dtype=torch.float32, device=self.device)\n",
    "                    label_label_loss = defaultdict(list)\n",
    "                    label_unlabel_loss = defaultdict(list)\n",
    "                    \n",
    "                    try:\n",
    "                        batch = random.sample(train_node_pairs_cpy, batch_size)\n",
    "                    except ValueError:\n",
    "                        break\n",
    "                        \n",
    "                    \n",
    "                    #labelled-labelled\n",
    "                    first_batch = [(src, dest) for (src, dest) in batch if (src in labeled_nodes and dest in labeled_nodes)]\n",
    "                    src_vectors = torch.tensor([node2vec[src] for (src, dest) in first_batch], device=self.device)\n",
    "                    dest_vectors = torch.tensor([node2vec[dest] for (src, dest) in first_batch], device=self.device)\n",
    "                    src_targets = torch.tensor([labeled_nodes[src] for (src, dest) in first_batch], device=self.device)\n",
    "                    dest_targets = torch.tensor([labeled_nodes[src] for (src, dest) in first_batch], device=self.device)\n",
    "                    if len(src_vectors) > 1:\n",
    "                        src_softmax = self.forward(src_vectors)\n",
    "                        l1_loss = self.aggregate_ce(src_softmax, src_targets)\n",
    "                        dest_softmax = self.forward(dest_vectors)\n",
    "                        l2_loss = self.aggregate_ce(dest_softmax, dest_targets)\n",
    "                        # TO DO: GET INCIDENT EDGES TO SRC AND DEST\n",
    "                        loss += (l1_loss + l2_loss)\n",
    "                        for s, d in zip(src_softmax, dest_softmax):\n",
    "                            total_loss += torch.dist(s, d) # TO DO: WEIGHTS\n",
    "                        \n",
    "                        \n",
    "                    #labelled-unlabelled\n",
    "                    second_batch = [(src, dest) for (src, dest) in batch if (src in labeled_nodes and (dest not in labeled_nodes))]\n",
    "                    src_vectors = torch.tensor([node2vec[src] for (src, dest) in second_batch], device=self.device)\n",
    "                    dest_vectors = torch.tensor([node2vec[dest] for (src, dest) in second_batch], device=self.device)\n",
    "                    src_targets = torch.tensor([labeled_nodes[src] for (src, dest) in second_batch], device=self.device)\n",
    "                    if len(src_vectors) > 1:\n",
    "                        src_softmax = self.forward(src_vectors)\n",
    "                        #print(\"vectors\", src_vectors, \"targets\", src_targets)\n",
    "                        l1_loss = self.aggregate_ce(src_softmax, src_targets)\n",
    "                        dest_softmax = self.forward(dest_vectors)\n",
    "                        # TO DO: GET INCIDENT EDGES TO SRC \n",
    "                        total_loss += (l1_loss)\n",
    "                        for s, d in zip(src_softmax, dest_softmax):\n",
    "                            loss += torch.dist(s, d) # TO DO: WEIGHTS\n",
    "                        \n",
    "                        \n",
    "                    #unlabelled-unlabelled\n",
    "                    third_batch = [(src, dest) for (src, dest) in batch if ((src not in labeled_nodes) and (dest not in labeled_nodes))]\n",
    "                    src_vectors = torch.tensor([node2vec[src] for (src, dest) in third_batch], device=self.device)\n",
    "                    dest_vectors = torch.tensor([node2vec[dest] for (src, dest) in third_batch], device=self.device)\n",
    "                    if len(src_vectors) > 1:\n",
    "                        src_softmax = self.forward(src_vectors)\n",
    "                        dest_softmax = self.forward(dest_vectors)\n",
    "                        for s, d in zip(src_softmax, dest_softmax):\n",
    "                            loss += torch.dist(s, d) # TO DO: WEIGHTS\n",
    "                    \n",
    "                    train_node_pairs_cpy = [i for i in train_node_pairs_cpy if i not in batch]\n",
    "                    \n",
    "                   \n",
    "                    if loss.item() != 0:\n",
    "                        assert not torch.isnan(loss)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        total_loss += loss.item()\n",
    "                        del loss\n",
    "\n",
    "                avg_loss = total_loss / len(labeled_nodes)\n",
    "                print(\"Epoch: {} Loss: {} (avg: {})\".format(e + 1, total_loss, avg_loss))\n",
    "\n",
    "         \n",
    "\n",
    "    def predict(self, tf_idf_vec):\n",
    "        return torch.argmax(self.forward(tf_idf_vec)).item()\n",
    "        \n",
    "    def evaluate(self, test_nodes, node2vec, node2label):\n",
    "        self.eval()\n",
    "\n",
    "        correct_count = 0\n",
    "        for node in test_nodes:\n",
    "            predicted = self.predict(torch.tensor(node2vec[node], device=self.device))\n",
    "            if predicted == node2label[node]:\n",
    "                correct_count += 1\n",
    "\n",
    "        return float(correct_count) / len(test_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline feed-forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "================================================================================\n",
      "Generation: 1 (with 60 labeled nodes)\n",
      "Epoch: 1 Loss: 77.44475434720516 (avg: 1.2907459057867527)\n",
      "Epoch: 2 Loss: 64.01455010473728 (avg: 1.066909168412288)\n",
      "Epoch: 3 Loss: 54.67511613667011 (avg: 0.9112519356111686)\n",
      "Epoch: 4 Loss: 47.92311353236437 (avg: 0.7987185588727395)\n",
      "Epoch: 5 Loss: 43.2147398814559 (avg: 0.7202456646909317)\n",
      "Epoch: 6 Loss: 39.25141831487417 (avg: 0.6541903052479029)\n",
      "Epoch: 7 Loss: 35.82105325907469 (avg: 0.5970175543179115)\n",
      "Epoch: 8 Loss: 32.8569213822484 (avg: 0.5476153563708067)\n",
      "Epoch: 9 Loss: 30.23717414587736 (avg: 0.5039529024312893)\n",
      "Epoch: 10 Loss: 27.92315060645342 (avg: 0.4653858434408903)\n",
      "Epoch: 11 Loss: 25.87398335337639 (avg: 0.4312330558896065)\n",
      "Epoch: 12 Loss: 24.059869810938835 (avg: 0.40099783018231394)\n",
      "================================================================================\n",
      "Generation: 2 (with 60 labeled nodes)\n",
      "Epoch: 1 Loss: 22.470564048737288 (avg: 0.3745094008122881)\n",
      "Epoch: 2 Loss: 21.026044607162476 (avg: 0.35043407678604127)\n",
      "Epoch: 3 Loss: 19.73231590911746 (avg: 0.32887193181862434)\n",
      "Epoch: 4 Loss: 18.56782630458474 (avg: 0.30946377174307904)\n",
      "Epoch: 5 Loss: 17.515417397022247 (avg: 0.29192362328370414)\n",
      "Epoch: 6 Loss: 16.54284182935953 (avg: 0.2757140304893255)\n",
      "Epoch: 7 Loss: 15.656729958951473 (avg: 0.26094549931585787)\n",
      "Epoch: 8 Loss: 14.839003086090088 (avg: 0.24731671810150146)\n",
      "Epoch: 9 Loss: 14.083544854074717 (avg: 0.23472574756791195)\n",
      "Epoch: 10 Loss: 13.385323513299227 (avg: 0.22308872522165377)\n",
      "Epoch: 11 Loss: 12.744166627526283 (avg: 0.21240277712543806)\n",
      "Epoch: 12 Loss: 12.131837207823992 (avg: 0.20219728679706653)\n"
     ]
    }
   ],
   "source": [
    "# Important variable from previous cells: node_pairs, node2vec, node2label, seed_nodes, train_node_pairs, test_nodes\n",
    "from datetime import datetime\n",
    "baseline_model = MY_NGM_FFNN(0, TF_IDF_DIM, HIDDEN_1_DIM, HIDDEN_2_DIM, NUM_CATEGORIES)\n",
    "start = datetime.now()\n",
    "baseline_model.train_(seed_nodes, train_node_pairs, node2vec, node2label, NUM_EPOCH, BATCH_SIZE, LEARNING_RATE)\n",
    "baseline_time = (datetime.now()-start).total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural graph machine feed-forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Important variable from previous cells: node_pairs, node2vec, node2label, seed_nodes, train_node_pairs, test_nodes\n",
    "\n",
    "NGM_model = NGM_FFNN(ALPHA, TF_IDF_DIM, HIDDEN_1_DIM, HIDDEN_2_DIM, NUM_CATEGORIES)\n",
    "start = datetime.now()\n",
    "NGM_model.train_(seed_nodes, train_node_pairs, node2vec, node2label, NUM_EPOCH, BATCH_SIZE, LEARNING_RATE)\n",
    "NGM_time = (datetime.now()-start).total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507.903482\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'NGM_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-207-fbe27ac8d87d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNGM_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'NGM_time' is not defined"
     ]
    }
   ],
   "source": [
    "print(baseline_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Important variable from previous cells: node2vec, node2label, test_nodes\n",
    "\n",
    "print(baseline_model.evaluate(test_nodes, node2vec, node2label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Model saved.\n",
      "Saving model...\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "baseline_model.save(MODEL_DIR, \"PubMed_baseline\")\n",
    "NGM_model.save(MODEL_DIR, \"PubMed_NGM\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
